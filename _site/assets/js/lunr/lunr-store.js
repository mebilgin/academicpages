var store = [{
        "title": "Domain Prediction for Open Domain QA",
        "excerpt":"Download PDF here. This article is still in the writing and testing phase. The experimental and algorithm results of the article have not been formed yet. Article details will soon be available on ArXiv. Please see the abstract part for the foreground for now. Abstract: abstract text will be here....","categories": ["articles"],
        "tags": ["domain adaptation","domain prediction","question answering","nlp","natural language processing"],
        "url": "/articles/2022-08-20-domain-prediction/",
        "teaser": null
      },{
        "title": "Topic Driven Question Generation",
        "excerpt":"Download PDF here. This article is still in the writing and testing phase. The experimental and algorithm results of the article have not been formed yet. Article details will soon be available on ArXiv. Please see the abstract part for the foreground for now. Abstract: abstract text will be here....","categories": ["articles"],
        "tags": ["natural language processing","nlp","question generation","topic driven"],
        "url": "/articles/2022-08-20-topic-driven-question-generation/",
        "teaser": null
      },{
        "title": "Tensors",
        "excerpt":"    ","categories": ["posts"],
        "tags": ["tensor"],
        "url": "/posts/2022/06/tensors/",
        "teaser": null
      },{
        "title": "Factorization and Decomposition",
        "excerpt":"     ","categories": ["posts"],
        "tags": ["tensor"],
        "url": "/posts/2022/08/factor-decomp/",
        "teaser": null
      },{
        "title": "Introduction JAX",
        "excerpt":"    ","categories": ["posts"],
        "tags": ["jax","tensor","tensorflow","pytorch"],
        "url": "/posts/2022/11/introduction-jax/",
        "teaser": null
      },{
        "title": "Statistical Perspective on Machine Learning",
        "excerpt":"Link to slides In this session, we talked about the equivalent of statistics and probability science in machine learning, which pieces of the puzzle represent, and probabilistic and statistical perspectives. During the session, we talked about Markov model, Monte Carlo model, Markov Chain Monte Carlo, Metropolis-Hastings and Viterbi Algorithms. We...","categories": [],
        "tags": [],
        "url": "/talks/arel-ml/",
        "teaser": null
      },{
        "title": "Computational Machine Learning",
        "excerpt":"Link to slides We built the foundations of this meeting on computational methods in machine learning. Tensor, Hadoop, Distributed Machine Learning and Application and Optimization Methods; Convex and Concave functions In this talk, I talked about Tensor methods in the section reserved for me. I talked about the place of...","categories": [],
        "tags": [],
        "url": "/talks/iu-comp-ml/",
        "teaser": null
      },{
        "title": "How Deep is Deep Learning?",
        "excerpt":"Link to slides In this talk, we talked about traditional machine learning methods and deep learning methods and the differences between them. I talked about the differences, advantages, disadvantages and applications of deep learning from traditional machine learning. I also talked about the new perspectives and new research areas that...","categories": [],
        "tags": [],
        "url": "/talks/iu-how-deep/",
        "teaser": null
      },{
        "title": "Start to End Machine Learning",
        "excerpt":"Link to slides In this meeting, we talked about the milestones that machine learning has passed from its beginning to the present and the level we have reached. We evaluated the first stone that Turing threw into the water and the subsequent developments chronologically. Then we talked about all the...","categories": [],
        "tags": [],
        "url": "/talks/iu-start-end-ml/",
        "teaser": null
      },{
        "title": "Graph Neural Network and Applications",
        "excerpt":"Link to slides   In this talk, we talked about Graph Neural Networks and Applications. In the speech, graph from various methods of neural networks; we talked about methods such as classification, embeddings, attention.  ","categories": [],
        "tags": [],
        "url": "/talks/iu-gnn-end-app/",
        "teaser": null
      },{
        "title": "Data Scientist and ML Engineer Career RoadMap",
        "excerpt":"Link to slides In this talk, we talked about Data Scientist and ML Engineer Career RoadMap and Difficulties Will Face. We talked about where and for what math, statistics/probability, algebra and linear algebra subjects are needed by Data Scientists and ML Engineers, how Data teams are formed and the steps...","categories": [],
        "tags": [],
        "url": "/talks/beykent-roadmap/",
        "teaser": null
      },{
        "title": "08-05-2022 Daily Reading Log",
        "excerpt":"Today, I mostly read on monte carlo with tensor and optimization techniques. You can find the resources I read below. Scalable Second Order Optimization for Deep Learning Tensor train completion: local recovery guarantees via Riemannian optimization Why Machine Learning Cannot Ignore Maximum Likelihood Estimation Monte Carlo simulation with tensor network...","categories": [],
        "tags": [],
        "url": "/teaching/2022-05-08-daily-reading-log/",
        "teaser": null
      },{
        "title": "10-05-2022 Daily Reading Log",
        "excerpt":"Today, I mostly read on monte carlo with tensor and optimization techniques. You can find the resources I read below.      Dan Jurafsky: How AI is changing our understanding of language   Hamiltonian Monte Carlo explained   A Recipe for Training Neural Networks   The Ottoman Text Recognition Network   Deep learning model compression  ","categories": [],
        "tags": [],
        "url": "/teaching/2022-05-10-daily-reading-log/",
        "teaser": null
      },{
        "title": "11-05-2022 Daily Reading Log",
        "excerpt":"Today, I mostly read on monte carlo with tensor and optimization techniques. You can find the resources I read below.      Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data   Deep contextualized word representations   Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora   ","categories": [],
        "tags": [],
        "url": "/teaching/2022-05-11-daily-reading-log/",
        "teaser": null
      },{
        "title": "21-09-2022 Daily Reading Log",
        "excerpt":"Due to my business life, I could not find time to read, write and produce for a long time. During this time, besides the tiredness of business life, I took some time to listen to myself. Thatâ€™s why I decided to write my first blog post Creating and producing time....","categories": [],
        "tags": [],
        "url": "/teaching/2022-09-21-daily-reading-log/",
        "teaser": null
      }]
